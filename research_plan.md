#+ Research Plan: Prompt Injection in Agentic RAG Systems

## Abstract
This plan outlines a thesis‑level, systematic investigation of prompt injection vulnerabilities in agentic Retrieval‑Augmented Generation (RAG) systems and the efficacy/overhead of layered defenses. We leverage the full capabilities of this codebase: core RAG pipeline, multiple retrieval modes, an agentic ReACT loop with tools, a comprehensive attack framework (single‑ and multi‑turn), a modular defense framework (input sanitization, prompt engineering, output validation, tool guardrails), and execution traces. We specify research questions, threat model, metrics, experimental factors, datasets, experiments (goals + implementations), analysis, reproducibility, and safety.

## Research Questions
- RQ1: How susceptible are agentic RAG systems to different injection vectors (corpus, context, system bypass, tool injection, multi‑step)?
- RQ2: Which defenses (alone and combined) reduce attack success rates (ASR) and with what cost (false positives, latency, utility)?
- RQ3: How do retrieval method choices (vector/keyword/hybrid) and tool availability influence vulnerability and defense effectiveness?
- RQ4: What attack parameters (payload style, location/placement, intensity, sequence length) most affect success?

## Threat Model & Attack Taxonomy
Adversary manipulates inputs at multiple layers:
- Corpus poisoning: injects malicious instructions in documents retrieved by RAG
- Context injection: boundary breaking, context stuffing, cross‑context leakage
- System bypass: role‑playing, instruction override, delimiter escape
- Tool injection: injected tool calls, parameter manipulation, chaining, unauthorized tools
- Multi‑step attacks: goal hijacking, reasoning poisoning, state corruption, observation injection

Mapping to code: see `src/attacks/*` and dataset generation in `src/attacks/attack_generator.py`.

## System Capabilities Used
- RAG core: embeddings, SQLite vector DB (vector/FTS5 BM25/hybrid search), prompt building, llama‑cpp LLM wrapper
- Agentic layer: ReACT loop, ToolRegistry, ToolParser, ToolExecutor
- Tools: `calculator`, `file_reader` (intentional attack vector), `web_search`, `database_query` (mock)
- Attacks: all categories above
- Defenses: `InputSanitization`, `PromptEngineeringDefense`, `OutputValidator`, `ToolGuardrails`, orchestrated by `DefenseManager`

## Metrics
- Attack Success Rate (ASR): proportion of attacks detected as successful (per attack’s `measure_success`)
- Defense Effectiveness: 1 − ASR(defended)/ASR(baseline)
- False Positive Rate (FPR): benign queries blocked/altered by defenses
- Semantic Preservation: correctness/consistency on benign QA (utility with defenses)
- Performance Overhead: added latency and tokens from defenses
- Tool Misuse Rate: unauthorized tool calls that executed

## Experimental Factors
- Attack type/variant: all in `src/attacks/*` (see README)
- Defenses: off vs. individual vs. combinations (via `src/defenses/defense_manager.py` and `config/defense_config.yaml`)
- Retrieval: `vector`, `keyword` (BM25), `hybrid` (`alpha` sweep)
- Agentic mode: pipeline‑only vs. agent with tools
- Tools: dangerous set enabled vs. restricted whitelist (via `config/agent_config.yaml` and defense guardrails)
- Attack intensity: poison ratio, placement, payload length, multi‑turn length

## Datasets
- Clean corpus: neutral texts under `data/corpus`
- Poisoned corpora: created via `CorpusPoisoningAttack.poison_corpus(...)` into `collection_id='poisoned'`
- Attack payload sets: generated by `AttackGenerator` (single‑turn) and by multi‑step `MultiStepAttack`

## Baseline Setup
1) Ingest corpora with `src/utils/document_ingestion.py`.
2) Build pipeline and agent with/without defenses:
- Pipeline: `create_rag_pipeline(db_path, embedding_model_path, llm_model_path, defense_manager=...)`
- Agent: `create_react_agent(rag_pipeline, tools=[...], agent_config_path, defense_config_path)`
3) Deterministic settings: low temperature, fixed seeds, controlled tool sets.

## Experiments (Goal + Implementation)

E1. Baseline ASR by attack type
- Goal: Establish vulnerability across attack categories with defenses off.
- Impl: Disable defenses; enable dangerous tools; generate N payloads/variant using `AttackGenerator.generate_attack_dataset`; run via pipeline `query(...)` (or agent `run(...)`); compute ASR via each attack’s `measure_success(response)`.

E2. Defense efficacy (individual)
- Goal: Measure how each defense reduces ASR.
- Impl: Toggle one defense at a time in `config/defense_config.yaml` (`defense_types.*.enabled: true`), rebuild `DefenseManager` and repeat E1; collect ASR deltas and reasons from sanitizer/validator outputs.

E3. Defense combinations & synergy
- Goal: Identify minimal effective bundles and interactions.
- Impl: Enable pairs/triples (e.g., PromptEngineering + OutputValidation; OutputValidation + ToolGuardrails; all four); compare ASR/FPR/latency vs. single defenses.

E4. Retrieval method sensitivity
- Goal: Assess vulnerability by retrieval mode.
- Impl: Sweep `retrieval_method` in pipeline queries (`vector`/`keyword`/`hybrid`) and `alpha` for hybrid; compute ASR per method; log retrieved contexts content.

E5. Agentic vs non‑agentic
- Goal: Attribute risk added by agent/tool calling.
- Impl: Compare pipeline `query()` vs agent `run()` on identical attacks; track `trace` for tool misuse; compute deltas in ASR and tool misuse rate.

E6. Tool injection & guardrails
- Goal: Validate `ToolGuardrails` under injected/unauthorized tool attacks.
- Impl: Attack type `tool_injection`; toggle guardrails whitelist and parameter validation; measure blocked vs allowed calls; false blocks on benign tool use.

E7. Poison severity (ratio/placement)
- Goal: ASR vs poison ratio and placement.
- Impl: Use `poison_corpus()` with multiple `poison_ratio` and `poison_placement`; query `collection_id='poisoned'`; compute ASR curves.

E8. Prompt engineering defenses
- Goal: Test delimiter wrapping and instructional hierarchy against context injection.
- Impl: Enable `PromptEngineeringDefense`; evaluate boundary‑breaking and stuffing variants; check if outputs still reflect injected tokens/instructions.

E9. Approval workflow (agent)
- Goal: Impact of approval gating on tool misuse.
- Impl: Set `require_approval: true` (research prompt) or simulate approvals; measure tool misuse drop vs throughput penalty.

E10. Context stuffing length thresholds
- Goal: Determine stuffing sizes that overcome defenses.
- Impl: Vary repetition/length of stuffing payloads; record ASR vs. length and prompt token counts.

E11. False positives & semantic preservation
- Goal: Ensure defenses preserve benign QA quality.
- Impl: Evaluate a benign query set on clean corpus; measure correctness/consistency deltas and FPR under defenses.

E12. Performance overhead
- Goal: Quantify latency and token overhead introduced by defenses.
- Impl: Time defense application and end‑to‑end latencies; compute prompt token counts; compare to baseline.

## Reference Implementations
- Ingestion: `src/utils/document_ingestion.py`
- Pipeline: `src/core/rag_pipeline.py` (with optional `DefenseManager`)
- Agent: `src/agent/react_agent.py` (with optional `DefenseManager`)
- Tool registry from config: `src/agent/tool_registry.py:build_tool_registry_from_config`
- Defenses orchestration: `src/defenses/defense_manager.py`
- Attacks: `src/attacks/*`, generator `src/attacks/attack_generator.py`

## Analysis Plan
- Confidence intervals for ASR/FPR/utility (bootstrap). Paired comparisons between defense conditions (e.g., McNemar/paired bootstrap). Factorial assessment of interactions where feasible. Summarize overhead distributions.

## Reproducibility
- Fix seeds; log configs and environment; maintain raw per‑sample traces; keep attack datasets under `data/attack_payloads/` or `experiments/`.

## Safety & Ethics
- Keep dangerous tools whitelisted to safe directories (`config/agent_config.yaml`) and/or guardrailed; `database_query` remains mock‑only. Do not enable real external execution.

## Timeline & Milestones
- Week 1: Baseline ingestion, E1/E2/E3 scaffolding
- Week 2: Retrieval & agentic deltas (E4/E5), guardrails study (E6)
- Week 3: Poison severity (E7), prompt engineering (E8), approval (E9)
- Week 4: Stuffing thresholds (E10), benign QA (E11), overhead (E12)
- Week 5: Analysis, report writing, replication pack

---

## Appendix A: Minimal Code Patterns
- Build DM + agent
```python
from src.defenses.defense_manager import create_defense_manager
from src.agent.react_agent import create_react_agent

defense_mgr = create_defense_manager()
agent = create_react_agent(rag_pipeline=pipeline, tools=['calculator','file_reader'], defense_config_path='config/defense_config.yaml')
```
- Run dataset
```python
from src.attacks.attack_generator import AttackGenerator

ag = AttackGenerator()
dataset = ag.generate_attack_dataset(['corpus_poisoning','tool_injection'], num_samples_per_variant=20)
for s in dataset:
    out = pipeline.query(s['payload'], retrieval_method='hybrid', k=3)
    # check success using corresponding attack class
```
- Multi‑step
```python
from src.attacks.multi_step_attacks import MultiStepAttack
atk = MultiStepAttack(variant='goal_hijacking')
seq = atk.generate_attack_sequence(num_turns=5)
results = atk.execute_multi_turn(agent, queries=seq, num_turns=5)
```
